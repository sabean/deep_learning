{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: House Prices using Regression\n",
    "\n",
    "This notebook will focus on the application of Neural Networks for house price prediction. Instead of images as pixel-wise inputs, we will consider generic values that describe various features for houses.\n",
    "\n",
    "![teaser](images/teaser.jpg)\n",
    "\n",
    "In our previous exercises we omitted a detailed data preperation overview, as it was not the main focus of those exercises since it was disctracting your attention from the basics of neural networks. In this notebook, we will optionally look a little more in-depth in data analysis. In the end, the class will not focus tremendously on these tasks but they will be essential routines that you will encounter if you choose to work in the area of deep/machine learning.\n",
    "\n",
    "The task itself is a *regression* problem and we will give you a few hinters how you can extend your build deep learning library of the first notebook to solve it, but you can also explore it as a *classification* task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Warning</h3>\n",
    "    <p>We will not explore neural network implementations in this exercise. If you want to train a network in the end, we suggest to work on \"1_FullyConnectedNets.ipynb\" first before starting the neural network part of this submission as we will be using the fully connected neural network class of the mentioned notebook.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Data\n",
    "## Exploration\n",
    "\n",
    "Make sure to run the *download_datasets.sh* script first before running the upcoming cell. Previously, we provided you with a data loading wrapper function to access the CIFAR10 data. This time around, our input is a csv file which we will load ourselves using [pandas](https://pandas.pydata.org) where we can easily access and alter entries in our data matrix. Let's have a small glimpse how the data looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"datasets/house_prices_data.csv\")\n",
    "labels = pd.read_csv(\"datasets/house_prices_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can easily get an overview of our features using .info(). Note that not all features are actually numbers!\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the describe function we can get an overview about numerical ranges\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target variable is the SalesPrice\n",
    "# The pandas dataframe string it is data_train\n",
    "# We explore it here\n",
    "labels['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply operations to a small subset of our data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smaller test data frames\n",
    "data_small = data[:5]\n",
    "labels_small = labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "In comparison to the previous notebooks, we always provided the data and label transforms for your convenience. However, if you want to apply your model to real life scenarios, you will have to provide a transform function that prepares raw data for your network. We already wrote a short transform class for you which will provide initial data augmenations for you under **exercise_code/transforms.py**. However, if you want to improve your network's performance, you will have to edit this class and change it to your liking!\n",
    "\n",
    "In the end, you will not only submit your network model file but als your transform class! Using this class we can alter the test data as well as labels so that your neural network can correctly classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the transform class\n",
    "from exercise_code.transforms import Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data & Non-Numerical Values\n",
    "\n",
    "Real life data is usually not perfect. There might be unreasonable or even missing entries. Let us first check out missing entries in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore missing data\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# Only show top 20 entries\n",
    "missing_data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, there are some columns that contains string information which can not parsed that easily. In order to use this information, one has to transform them into a (for the network) read-able format. \n",
    "\n",
    "For the initial setup, we will do the following two steps:\n",
    "- we will omit non-numerical columns\n",
    "- we will set all numerical missing values to 0\n",
    "\n",
    "Those decisions are obviously not optimal. You are free to explore how one should use non-numerical entries to your advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Inline Question</h3>\n",
    "    <p>What solution would you propose to handle missing numerical values? Do you think we can just transform non-numerical values to integers or should one do something more elaborate? In addition to that there might be new non-numerical entries that are not present in the training set. What would you propose how we should handle those?</p>\n",
    "    <p>**Your answer:** </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook, we are performing a regression here for numerical attributes only\n",
    "data_small = Transforms.get_only_numeric_attributes(data_small, verbose=True)\n",
    "labels_small = Transforms.get_only_numeric_attributes(labels_small, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we lost quite a bit of our features and are only left with 36 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "As we have seen in our batch normalization notebook, it is of utmost important to properly normalize the input values for our network. We use our provided function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Let us normalize the dataset in order to allow faster convergence\n",
    "data_small = Transforms.min_max_scalar(data_small)\n",
    "labels_small = Transforms.min_max_scalar(labels_small, is_labels_column=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the min and max values that were learned on the training set,\n",
    "# we are going to apply the same on the validation set\n",
    "data_small = Transforms.min_max_scalar(data_small, invoker='data')\n",
    "labels_small = Transforms.min_max_scalar(labels_small, invoker='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished Transform Class\n",
    "\n",
    "Our final transform class should have a single call to transform data as well as labels. This function will be called by our test as well and you should change it if you want to apply other transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  a new small test set first\n",
    "data_small = data[:5]\n",
    "labels_small = labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the full transform function for data as well as labels\n",
    "prepared_data_small = Transforms.apply_data_transforms(data_small)\n",
    "prepared_labels = Transforms.apply_labels_transforms(labels_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a short look to compare our prepared labels\n",
    "print(\"Original labels:\", labels_small)\n",
    "print(\"Normalized labels:\", prepared_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "As for every training task we have to split up our provided data to validate our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the split on data to create train and validation set\n",
    "X_train = data[:800]\n",
    "X_val = data[800:]\n",
    "y_train = labels[:800]\n",
    "y_val = labels[800:]\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the **min_max_scalar** function of the **Transforms** class. This function scales all values between zero and one as one can do for images. The difference here is that -- while images have a fixed minimum and maximum in 0 and 255 -- we don't know the theoretical minima and maxima for your data and thus our function is evoked differently for the training and validation/test set. \n",
    "\n",
    "You have to take those things into account for all transforms you'd like to implement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Transforms.apply_data_transforms(X_train, mode='train')\n",
    "y_train = Transforms.apply_labels_transforms(y_train, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes of the training data\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert the train data using our transform class\n",
    "X_train, min_value, max_value = Transforms.apply_data_transforms(X_train)\n",
    "y_train, min_value, max_value = Transforms.apply_labels_transforms(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the validation data using our now initialized transform class\n",
    "X_val = Transforms.apply_data_transforms(X_val,mode='val')\n",
    "y_val = Transforms.apply_labels_transforms(y_val, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew, that was a lot of work, but we are now done with our data preperation for now and can move to the actual network part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next _optional_ steps:\n",
    "\n",
    "## Classification of Mean\n",
    "\n",
    "We are now ready to train networks. As a first step, you can simplify the task to a simple classification task that we can solve using our network structure from the first notebook.\n",
    "\n",
    "In order to to this, we have to use different labels. We will alter our labels such that we return 1 if the entry is bigger than the mean of our training set or 0 if it is smaller. This can be done easily with our transform function.\n",
    "\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the labels into binary values since this is a classification task\n",
    "def convert_to_binary_label(input_vector):\n",
    "    \"\"\"\n",
    "    :param input_vector: a vector of real numbers\n",
    "    :return: vector of 0,1 depending upon if the value is greater than mean value of the vector\n",
    "    \"\"\"\n",
    "    mean_value = np.mean(input_vector)\n",
    "    label_vector = np.array(input_vector > mean_value)\n",
    "    return label_vector.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = convert_to_binary_label(y_train)\n",
    "y_val_binary = convert_to_binary_label(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values into a dictionary of the required format\n",
    "# This dictionary can be fed into the NN\n",
    "input_data = Transforms.prepare_dictionary(X_train, X_val, y_train_binary, y_val_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "Now it is your turn! Initialize a neural network as in the first notebook using the **FullyConnectedNet** class that you wrote prior. This task should not be that hard but as a sanity check you should first try to overfit on a smaller data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the previously used solver and network classes\n",
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.fc_net import *\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first look at: Regression\n",
    "\n",
    "Previously, we approached the problem similarly to our previous tasks, i.e., we considered it like a classification problem. We can approximate the actual value by using more fine grained buckets but we have a hard time predicting an actual value.\n",
    "\n",
    "For this task, we will explore regression to directly predict the actual numerical value. We have to make some changes to our loss function to use the **l2 loss**. Please take a look at the `l2_loss` function in `layers.py` for the implementation.\n",
    "\n",
    "There are also some updates for the solver. As we directly predict float values, there is no easy notion of \"accuracy\". Thus, we will only consider the loss value and look for the model with the smallest loss.  Please take a look at `update_accuracy` function in `solver.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values into a dictionary of the required format\n",
    "# This dictionary can be fed into the NN\n",
    "input_data_for_NN = Transforms.prepare_dictionary(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "When you are satisfied with your training, you can save the model but there will be no submission for this notebook. The required number of submissions for this class have been reduced to 6 from 7.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Warning</h3>\n",
    "    <p>You might get an error like this:</p>\n",
    "    <p>PicklingError: Can't pickle <class 'exercise_code.classifiers.softmax.SoftmaxClassifier'>: it's not the same object as exercise_code.classifiers.softmax.SoftmaxClassifier</p>\n",
    "    <p>The reason is that we are using autoreload and working on this class during the notebook session. If you get this error simply restart the kernel and rerun the whole script (Kernel -> Restart & Run All) or only the important cells for generating your model.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.model_savers import save_fully_connected_net\n",
    "save_fully_connected_net(best_model, modelname='house_prices')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
